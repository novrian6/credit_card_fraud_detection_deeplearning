{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPf6zc1muRfukji7honCip5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/novrian6/credit_card_fraud_detection_deeplearning/blob/main/credit_card_fraud_identification_deeplearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##data: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/download?datasetVersionNumber=3\n"
      ],
      "metadata": {
        "id": "Ar2W6Fz5C8qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fraud detection using deep learning Binary classification\n"
      ],
      "metadata": {
        "id": "cL8R-fjABLFz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItLvyEfTvN1v",
        "outputId": "060033e2-115b-444c-a9de-0a9c775e857b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62    0.0  \n",
            "1  0.125895 -0.008983  0.014724    2.69    0.0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66    0.0  \n",
            "3 -0.221929  0.062723  0.061458  123.50    0.0  \n",
            "4  0.502292  0.219422  0.215153   69.99    0.0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "Epoch 1/10\n",
            "1904/1904 [==============================] - 8s 3ms/step - loss: 0.0141 - accuracy: 0.9979 - val_loss: 0.0020 - val_accuracy: 0.9993\n",
            "Epoch 2/10\n",
            "1904/1904 [==============================] - 5s 3ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0020 - val_accuracy: 0.9993\n",
            "Epoch 3/10\n",
            "1904/1904 [==============================] - 4s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0016 - val_accuracy: 0.9993\n",
            "Epoch 4/10\n",
            "1904/1904 [==============================] - 4s 2ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0022 - val_accuracy: 0.9991\n",
            "Epoch 5/10\n",
            "1904/1904 [==============================] - 5s 3ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0018 - val_accuracy: 0.9995\n",
            "Epoch 6/10\n",
            "1904/1904 [==============================] - 4s 2ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0018 - val_accuracy: 0.9993\n",
            "Epoch 7/10\n",
            "1904/1904 [==============================] - 4s 2ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0022 - val_accuracy: 0.9993\n",
            "Epoch 8/10\n",
            "1904/1904 [==============================] - 5s 3ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0019 - val_accuracy: 0.9994\n",
            "Epoch 9/10\n",
            "1904/1904 [==============================] - 4s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0025 - val_accuracy: 0.9995\n",
            "Epoch 10/10\n",
            "1904/1904 [==============================] - 4s 2ms/step - loss: 8.5573e-04 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9993\n",
            "595/595 [==============================] - 1s 1ms/step\n",
            "Accuracy: 0.9991068144801135\n",
            "Precision: 0.8372093023255814\n",
            "Recall: 0.782608695652174\n",
            "F1 Score: 0.8089887640449438\n",
            "Confusion Matrix:\n",
            " [[18980     7]\n",
            " [   10    36]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# Explore the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Splitting the data into features and target\n",
        "X = df.drop(['Time', 'Class'], axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Handle missing values in features\n",
        "imputer_X = SimpleImputer(strategy='mean')\n",
        "X = imputer_X.fit_transform(X)\n",
        "\n",
        "# Handle missing values in target\n",
        "imputer_y = SimpleImputer(strategy='most_frequent')\n",
        "y = imputer_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the neural network model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Convert predictions to binary\n",
        "y_test_binary = y_test\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
        "precision = precision_score(y_test_binary, y_pred_binary)\n",
        "recall = recall_score(y_test_binary, y_pred_binary)\n",
        "f1 = f1_score(y_test_binary, y_pred_binary)\n",
        "conf_matrix = confusion_matrix(y_test_binary, y_pred_binary)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Optimize for recall"
      ],
      "metadata": {
        "id": "feqyCjewwWGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# Splitting the data into features and target\n",
        "X = df.drop(['Time', 'Class'], axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Handle missing values in features\n",
        "imputer_X = SimpleImputer(strategy='mean')\n",
        "X = imputer_X.fit_transform(X)\n",
        "\n",
        "# Handle missing values in target\n",
        "imputer_y = SimpleImputer(strategy='most_frequent')\n",
        "y = imputer_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the neural network model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Adjusting threshold for classification to maximize recall\n",
        "y_pred_prob = model.predict(X_test)\n",
        "threshold = 0.6  # Adjust this threshold for desired recall\n",
        "y_pred = (y_pred_prob > threshold).astype(int)\n",
        "\n",
        "# Convert predictions to binary\n",
        "y_test_binary = y_test\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test_binary, y_pred)\n",
        "precision = precision_score(y_test_binary, y_pred)\n",
        "recall = recall_score(y_test_binary, y_pred)\n",
        "f1 = f1_score(y_test_binary, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test_binary, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9xIMy4Px9_S",
        "outputId": "8606f8e8-53df-4d6f-de89-3a5d20b61a44"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3687/3687 [==============================] - 12s 3ms/step - loss: 0.0094 - accuracy: 0.9988 - val_loss: 0.0025 - val_accuracy: 0.9994\n",
            "Epoch 2/10\n",
            "3687/3687 [==============================] - 8s 2ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0024 - val_accuracy: 0.9994\n",
            "Epoch 3/10\n",
            "3687/3687 [==============================] - 9s 3ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
            "Epoch 4/10\n",
            "3687/3687 [==============================] - 9s 3ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0027 - val_accuracy: 0.9992\n",
            "Epoch 5/10\n",
            "3687/3687 [==============================] - 8s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0025 - val_accuracy: 0.9995\n",
            "Epoch 6/10\n",
            "3687/3687 [==============================] - 9s 3ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0022 - val_accuracy: 0.9996\n",
            "Epoch 7/10\n",
            "3687/3687 [==============================] - 11s 3ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0027 - val_accuracy: 0.9991\n",
            "Epoch 8/10\n",
            "3687/3687 [==============================] - 8s 2ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0024 - val_accuracy: 0.9994\n",
            "Epoch 9/10\n",
            "3687/3687 [==============================] - 9s 2ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9996\n",
            "Epoch 10/10\n",
            "3687/3687 [==============================] - 9s 2ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0025 - val_accuracy: 0.9995\n",
            "1152/1152 [==============================] - 2s 1ms/step\n",
            "Accuracy: 0.999348923010146\n",
            "Precision: 0.9272727272727272\n",
            "Recall: 0.7183098591549296\n",
            "F1 Score: 0.8095238095238096\n",
            "Confusion Matrix:\n",
            " [[36787     4]\n",
            " [   20    51]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine tune with grid search cross validation"
      ],
      "metadata": {
        "id": "VISXzkD1yabw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# Splitting the data into features and target\n",
        "X = df.drop(['Time', 'Class'], axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Handle missing values in features\n",
        "imputer_X = SimpleImputer(strategy='mean')\n",
        "X = imputer_X.fit_transform(X)\n",
        "\n",
        "# Handle missing values in target\n",
        "imputer_y = SimpleImputer(strategy='most_frequent')\n",
        "y = imputer_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Custom KerasClassifier for GridSearchCV\n",
        "class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, build_fn=create_model, epochs=10, batch_size=32, verbose=0):\n",
        "        self.build_fn = build_fn\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = self.build_fn()\n",
        "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred_prob = self.model.predict(X)\n",
        "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "        return y_pred\n",
        "\n",
        "# Create a KerasClassifier for GridSearchCV\n",
        "keras_model = KerasClassifierWrapper(build_fn=create_model)\n",
        "\n",
        "# Define the parameters for Grid Search\n",
        "param_grid = {'epochs': [10, 20], 'batch_size': [32, 64]}\n",
        "\n",
        "# Create the Grid Search object\n",
        "grid_search = GridSearchCV(estimator=keras_model, param_grid=param_grid, scoring='recall', cv=3)\n",
        "\n",
        "# Fit the Grid Search to the training data\n",
        "grid_result = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters from the Grid Search\n",
        "best_epochs = grid_result.best_params_['epochs']\n",
        "best_batch_size = grid_result.best_params_['batch_size']\n",
        "\n",
        "# Train the model with the best parameters\n",
        "model = create_model()\n",
        "model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size, validation_split=0.2)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Best Epochs:\", best_epochs)\n",
        "print(\"Best Batch Size:\", best_batch_size)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNxHBECV11Ww",
        "outputId": "a03b4be3-d40e-4fbb-aff1-59e75ff24d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2374/2374 [==============================] - 3s 1ms/step\n",
            "2374/2374 [==============================] - 4s 2ms/step\n",
            "2374/2374 [==============================] - 8s 3ms/step\n",
            "2374/2374 [==============================] - 3s 1ms/step\n",
            "2374/2374 [==============================] - 4s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zlWfEQ6Y11nE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}